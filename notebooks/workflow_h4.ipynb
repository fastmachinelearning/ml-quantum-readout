{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# source /data/Xilinx_no_Vitis/Vivado/2020.1/settings64.sh\n",
    "!vivado -version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import pickle\n",
    "import hashlib\n",
    "import json\n",
    "\n",
    "import hls4ml \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "from sklearn.metrics import accuracy_score\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation, BatchNormalization, Dropout\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, ModelCheckpoint, EarlyStopping\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy, BinaryCrossentropy \n",
    "\n",
    "from tensorflow_model_optimization.python.core.sparsity.keras import prune, pruning_callbacks, pruning_schedule\n",
    "from tensorflow_model_optimization.sparsity.keras import strip_pruning\n",
    "import tensorflow_model_optimization as tfmot\n",
    "\n",
    "from qkeras.qlayers import QDense, QActivation\n",
    "from qkeras import QBatchNormalization\n",
    "from qkeras.quantizers import quantized_bits, quantized_relu\n",
    "from qkeras.utils import _add_supported_quantized_objects\n",
    "from tensorflow.keras.models import load_model\n",
    "from qkeras.utils import _add_supported_quantized_objects\n",
    "\n",
    "os.environ['PATH'] = os.environ['XILINX_VIVADO'] + '/bin:' + os.environ['PATH']\n",
    "keras.utils.set_random_seed(32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_array_to_dat(data, top_nrows, bottom_nrows, filename):\n",
    "    print('Save top {} rows and bottom {} rows in file {}'.format(top_nrows, bottom_nrows, filename))\n",
    "    with open(filename, 'w') as file:\n",
    "        if data.ndim == 1:\n",
    "            data_ = [[x] for x in data]\n",
    "        else:\n",
    "            data_ = data\n",
    "            \n",
    "        for row in data_[:top_nrows]:\n",
    "            file.write(' '.join(map(str, row)) + '\\n')\n",
    "        for row in data_[-bottom_nrows:]:\n",
    "            file.write(' '.join(map(str, row)) + '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_dir = \"../data/malab_05282024/npz/\"\n",
    "test_data_dir = \"../data/malab_05282024/npz/\"\n",
    "start_location = 100\n",
    "window_size = 400\n",
    "end_window = start_location + window_size # 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Loadning training split\"\"\"\n",
    "x_train_path = os.path.join(train_data_dir, f'0528_X_train_0_770.npy')\n",
    "y_train_path = os.path.join(train_data_dir, f'0528_y_train_0_770.npy')\n",
    "\n",
    "assert os.path.exists(x_train_path), f\"ERROR: File {x_train_path} does not exist.\"\n",
    "assert os.path.exists(y_train_path), f\"ERROR: File {y_train_path} does not exist.\"\n",
    "\n",
    "X_train_val = np.load(x_train_path)\n",
    "y_train_val = np.load(y_train_path)\n",
    "\n",
    "# Insure same dataset is loaded \n",
    "assert hashlib.md5(X_train_val).hexdigest() == 'b61226c86b7dee0201a9158455e08ffb',  \"Checksum failed. Wrong file was loaded or file may be corrupted.\"\n",
    "assert hashlib.md5(y_train_val).hexdigest() == 'c59ce37dc7c73d2d546e7ea180fa8d31',  \"Checksum failed. Wrong file was loaded or file may be corrupted.\"\n",
    "\n",
    "# Get readout window\n",
    "X_train_val = X_train_val[:,start_location*2:end_window*2]\n",
    "assert len(X_train_val[0]) == (end_window-start_location)*2, f\"ERROR: X_test sample size {len(X_train_val[0])} does not match (start window, end window) ({start_location},{end_window}) size.\"\n",
    "\n",
    "\n",
    "print(\"Train Data Set:\")\n",
    "print(f\"  X Path : {x_train_path}\")\n",
    "print(f\"  y Path : {y_train_path}\")\n",
    "print(f\"  Size : {len(X_train_val):,}\")\n",
    "print(f\"  Shape : {X_train_val[0].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Loading testing split\"\"\"\n",
    "x_test_path = os.path.join(test_data_dir, f'0528_X_test_0_770.npy')\n",
    "y_test_path = os.path.join(test_data_dir, f'0528_y_test_0_770.npy')\n",
    "\n",
    "assert os.path.exists(x_test_path), f\"ERROR: File {x_test_path} does not exist.\"\n",
    "assert os.path.exists(y_test_path), f\"ERROR: File {y_test_path} does not exist.\"\n",
    "\n",
    "X_test = np.load(x_test_path)\n",
    "y_test = np.load(y_test_path)\n",
    "\n",
    "# Insure same dataset is loaded \n",
    "assert hashlib.md5(X_test).hexdigest() == 'b7d85f42522a0a57e877422bc5947cde', \"Checksum failed. Wrong file was loaded or file may be corrupted.\"\n",
    "assert hashlib.md5(y_test).hexdigest() == '8c9cce1821372380371ade5f0ccfd4a2', \"Checksum failed. Wrong file was loaded or file may be corrupted.\"\n",
    "\n",
    "# Get readout window\n",
    "X_test = X_test[:,start_location*2:end_window*2]\n",
    "assert len(X_test[0]) == (end_window-start_location)*2, f\"ERROR: X_test sample size {len(X_test[0])} does not match (start window, end window) ({start_location},{end_window}) size.\"\n",
    "\n",
    "\n",
    "print(\"Test Data Set:\")\n",
    "print(f\"  X Path : {x_test_path}\")\n",
    "print(f\"  y Path : {y_test_path}\")\n",
    "print(f\"  Size : {len(X_test):,}\" )\n",
    "print(f\"  Sample Shape : {X_test[0].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build our model \n",
    "QKeras is \"Quantized Keras\" for deep heterogeneous quantization of ML models. We're using QDense layer instead of Dense. We're also training with model sparsity, since QKeras layers are prunable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_neurons = 4\n",
    "input_shape = int((end_window-start_location)*2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(input_shape, hidden=8, is_pruned=True, activation='sigmoid'):\n",
    "    model = keras.models.Sequential()\n",
    "    model.add(QDense(\n",
    "        hidden, \n",
    "        activation='relu', \n",
    "        name='fc1',\n",
    "        input_shape=(input_shape,), \n",
    "        kernel_quantizer=quantized_bits(3,0,alpha=1), bias_quantizer=quantized_bits(3,0,alpha=1)\n",
    "    ))\n",
    "    model.add(BatchNormalization(name='batchnorm1'))\n",
    "    # model.add(QBatchNormalization(\n",
    "    #         name='batchnorm1',\n",
    "    #         gamma_quantizer=quantized_bits(6, 0, 1),\n",
    "    #         mean_quantizer=quantized_bits(6, 6, 1),\n",
    "    #         variance_quantizer=quantized_bits(6, 6, 1),\n",
    "    #         beta_quantizer=quantized_bits(6, 0, 1),\n",
    "    # ))\n",
    "    model.add(\n",
    "        QDense(1, name='fc2', activation=activation, kernel_quantizer=quantized_bits(3,0,alpha=1), bias_quantizer=quantized_bits(3,0,alpha=1))\n",
    "    )\n",
    "\n",
    "    if is_pruned == True:\n",
    "        # adding pruning \n",
    "        pruning_params = {'pruning_schedule': tfmot.sparsity.keras.PolynomialDecay(initial_sparsity=0.10, final_sparsity=0.50, begin_step=100, end_step=500)}\n",
    "        model = prune.prune_low_magnitude(model, **pruning_params)\n",
    "    return model\n",
    "\n",
    "\n",
    "model = get_model(input_shape=input_shape, hidden=hidden_neurons, is_pruned=False)\n",
    "print(model.summary())\n",
    "print('Input shape:', input_shape)\n",
    "print('Number of hidden neurons:', hidden_neurons)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_learning_rate = 1e-3\n",
    "validation_split = 0.05  # 45,000 sample size \n",
    "batch_size = 256\n",
    "epochs = 100\n",
    "early_stopping_patience = 20\n",
    "checkpoint_dir = f'../checkpoints/scan_window_location_and_size_h{hidden_neurons}'\n",
    "checkpoint_filename = 'qkeras_model_best.h5'\n",
    "\n",
    "if os.path.exists(checkpoint_dir) == False:\n",
    "    print(f'Checkpoint directory {checkpoint_dir} does not exist.')\n",
    "    print('Creating directory...')\n",
    "    os.mkdir(checkpoint_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################\n",
    "# 0. init callbacks\n",
    "#########################\n",
    "ckp_dir = os.path.join(checkpoint_dir, f'sl{start_location}_ws{window_size}')\n",
    "if os.path.exists(ckp_dir) == False: os.mkdir(ckp_dir)\n",
    "print('Saving to', ckp_dir)\n",
    "\n",
    "ckp_filename = os.path.join(ckp_dir, checkpoint_filename)\n",
    "callbacks = [\n",
    "    ModelCheckpoint(\n",
    "        ckp_filename,\n",
    "        monitor=\"val_loss\",\n",
    "        verbose=0,\n",
    "        save_best_only=True,\n",
    "        save_weights_only=True,\n",
    "        save_freq=\"epoch\",\n",
    "    ),\n",
    "    EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=early_stopping_patience,\n",
    "        restore_best_weights=False,\n",
    "    ),\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    #########################\n",
    "    # 1. declare model \n",
    "    #########################\n",
    "    opt = Adam(learning_rate=init_learning_rate)\n",
    "    model = get_model(input_shape=window_size*2, hidden=hidden_neurons, is_pruned=False, activation='sigmoid')\n",
    "    model.compile(\n",
    "        optimizer=opt, \n",
    "        loss=BinaryCrossentropy(from_logits=False), \n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "\n",
    "    #########################\n",
    "    # 2. train \n",
    "    #########################\n",
    "    history = model.fit(\n",
    "        X_train_val, \n",
    "        y_train_val, \n",
    "        batch_size=batch_size,\n",
    "        epochs=epochs, \n",
    "        validation_split=validation_split, \n",
    "        shuffle=True, \n",
    "        callbacks=callbacks,\n",
    "    )\n",
    "\n",
    "    # Save the history dictionary\n",
    "    with open(os.path.join(ckp_dir, 'qkeras_training_history.pkl'), 'wb') as f:\n",
    "        pickle.dump(history.history, f)\n",
    "\n",
    "    #########################\n",
    "    # 3. compute fidelity \n",
    "    #########################\n",
    "    y_pred = model.predict(X_test)\n",
    "    test_acc = accuracy_score(y_test, np.where(y_pred <= 0.5, 0, 1).reshape(-1))\n",
    "\n",
    "    print('\\n===================================')\n",
    "    print(f'Start location = {start_location}, Window size = {window_size}')\n",
    "    print('    Accuracy', test_acc)\n",
    "    print('    Fidelity', test_acc*2-1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "notes = {}\n",
    "ckp_filename = os.path.join(ckp_dir, checkpoint_filename)\n",
    "ckp_filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_model = get_model(input_shape=input_shape, hidden=hidden_neurons, is_pruned=False, activation='sigmoid')\n",
    "checkpoint_model.load_weights(ckp_filename)\n",
    "\n",
    "y_keras = checkpoint_model.predict(X_test)\n",
    "test_acc = accuracy_score(y_test, np.where(y_keras < 0.5, 0, 1).reshape(-1))\n",
    "\n",
    "print(f\"Keras  Accuracy (with sigmoid): {test_acc}\")\n",
    "print(f\"Keras  Fidelity (with sigmoid): {test_acc*2-1}\")\n",
    "notes[\"Keras  Accuracy (with sigmoid)\"] = test_acc\n",
    "notes[\"Keras  Fidelity (with sigmoid)\"] = test_acc*2-1\n",
    "\n",
    "checkpoint_model = get_model(input_shape=input_shape, hidden=hidden_neurons, is_pruned=False, activation=None)\n",
    "checkpoint_model.load_weights(ckp_filename)\n",
    "\n",
    "y_pred = checkpoint_model.predict(X_test)\n",
    "test_acc = accuracy_score(y_test, np.where(y_pred < 0.5, 0, 1).reshape(-1))\n",
    "\n",
    "print(f\"Keras  Accuracy (w/o sigmoid): {test_acc}\")\n",
    "print(f\"Keras  Fidelity (w/o sigmoid): {test_acc*2-1}\")\n",
    "notes[\"Keras  Accuracy (w/o sigmoid)\"] = test_acc\n",
    "notes[\"Keras  Fidelity (w/o sigmoid)\"] = test_acc*2-1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check sparsity "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_layers = len(checkpoint_model.layers)\n",
    "print(f'Number of layers: {num_layers}')\n",
    "\n",
    "\n",
    "for idx in range(num_layers):\n",
    "    w = checkpoint_model.layers[idx].weights[0].numpy()\n",
    "    h, b = np.histogram(w, bins=100)\n",
    "    layer_sparsity = np.sum(w == 0) / np.size(w)\n",
    "\n",
    "    # plot weight distribution\n",
    "    plt.figure(figsize=(7, 7))\n",
    "    plt.bar(b[:-1], h, width=b[1] - b[0])\n",
    "    plt.semilogy()\n",
    "    plt.title(f'Layer {checkpoint_model.layers[idx].name}, {layer_sparsity:.2f} Sparsity')\n",
    "    plt.savefig(os.path.join(ckp_dir, f'model-dist-idx{idx}.png'))\n",
    "    print('% of zeros = {}'.format(layer_sparsity))\n",
    "\n",
    "\n",
    "checkpoint_model = strip_pruning(checkpoint_model)  # remove prune layers for hls4ml parsing \n",
    "checkpoint_model.save_weights(ckp_filename)  # save as weights only for keras tracing (cannot directly pass strip_pruned model)\n",
    "checkpoint_model = get_model(input_shape=input_shape, hidden=hidden_neurons, is_pruned=False, activation=None)\n",
    "checkpoint_model.load_weights(ckp_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HLS4ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys \n",
    "sys.path.append(\"../utils\")\n",
    "from config import print_dict\n",
    "\n",
    "from tensorflow.keras.models import load_model\n",
    "from qkeras.utils import _add_supported_quantized_objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create HLS configuration \n",
    "hls_config = {}\n",
    "hls_config['Model'] = {}\n",
    "hls_config['Model']['Precision'] = 'ap_fixed<16,6>'  # Default precision\n",
    "hls_config['Model']['ReuseFactor'] = 1  # parallelized \n",
    "hls_config['Model']['Strategy'] = 'Resource'\n",
    "\n",
    "hls_config['LayerName'] = {}\n",
    "keras_layers = ['fc1', 'fc1_relu', 'batchnorm1', 'fc2', 'fc2_linear']\n",
    "for layer in keras_layers:\n",
    "    hls_config['LayerName'][layer] = {}\n",
    "    hls_config['LayerName'][layer]['Precision'] = {}\n",
    "    hls_config['LayerName'][layer]['Trace'] = True\n",
    "\n",
    "# Input - ZCU216 uses 14-bit ADCS \n",
    "hls_config['LayerName']['fc1_input'] = {}\n",
    "hls_config['LayerName']['fc1_input']['Precision'] = {}\n",
    "hls_config['LayerName']['fc1_input']['Trace'] = False\n",
    "hls_config['LayerName']['fc1_input']['Precision'] = 'ap_fixed<14,14>' \n",
    "\n",
    "# Fc1\n",
    "hls_config['LayerName']['fc1']['Precision']['result'] = 'ap_fixed<19,18>'\n",
    "hls_config['LayerName']['fc1']['accum_t'] = 'ap_fixed<19,18>'\n",
    "\n",
    "# Fc1 activation \n",
    "hls_config['LayerName']['fc1_relu']['Precision']['result'] = 'ap_fixed<19,18>'\n",
    "\n",
    "# Batchnormalization\n",
    "hls_config['LayerName']['batchnorm1']['Precision']['scale'] = 'ap_fixed<18,2>'\n",
    "hls_config['LayerName']['batchnorm1']['Precision']['bias'] = 'ap_fixed<18,2>'\n",
    "hls_config['LayerName']['batchnorm1']['Precision']['result'] = 'ap_fixed<10,4>'\n",
    "hls_config['LayerName']['batchnorm1']['accum_t'] = 'ap_fixed<10,4>'\n",
    "\n",
    "# Fc2\n",
    "hls_config['LayerName']['fc2']['Precision']['result'] = 'ap_fixed<10,5>'\n",
    "hls_config['LayerName']['fc2']['accum_t'] = 'ap_fixed<10,5>'\n",
    "\n",
    "# Fc2 activation \n",
    "hls_config['LayerName']['fc2_linear']['Precision']['result'] = 'ap_fixed<10,5>'\n",
    "\n",
    "print_dict(hls_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build HLS model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = f'../hls4ml_projects/sl-{start_location}_ws-{window_size}_hn-{hidden_neurons}_Vivado'\n",
    "xilinx_part = 'xczu49dr-ffvf1760-2-e'\n",
    "io_type = 'io_parallel'\n",
    "clock_period = 3.225  # 3.225ns (307.2 MHz)\n",
    "hls_fig = os.path.join(output_dir, 'model.png')\n",
    "backend = 'Vivado' \n",
    "interface = 'axi_stream'\n",
    "#driver = 'c'\n",
    "board = 'zcu216'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hls_model = hls4ml.converters.convert_from_keras_model(\n",
    "    model=checkpoint_model,\n",
    "    hls_config=hls_config,\n",
    "    output_dir=output_dir,\n",
    "    part=xilinx_part,\n",
    "    io_type=io_type,\n",
    "    clock_period=clock_period,\n",
    "    backend=backend,\n",
    "    board=board,\n",
    "    interface=interface,\n",
    "    #driver=driver,\n",
    "    project_name='NN'\n",
    ")\n",
    "\n",
    "print(f\"Creating hls4ml project directory {output_dir}\")\n",
    "hls_model.compile()  # Must compile for C Sim. \n",
    "\n",
    "# Visualize model\n",
    "hls4ml.utils.plot_model(\n",
    "    hls_model, show_shapes=True, show_precision=True, to_file=hls_fig \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trace output \n",
    "y_hls = hls_model.predict(np.ascontiguousarray(X_test.astype(np.float32))) \n",
    "\n",
    "keras_acc = accuracy_score(y_test, np.where(y_keras < 0.5, 0, 1).reshape(-1))\n",
    "hls_acc = accuracy_score(y_test, np.where(y_hls < 0, 0, 1).reshape(-1))\n",
    "\n",
    "print(f'Keras Acc (w/ sigmoid): {keras_acc*100:.5}%')\n",
    "print(f'Keras Fidelity (w/ sigmoid): {(keras_acc*2-1) * 100:.5}%')\n",
    "print(f'HLS Acc: {hls_acc*100:.5}:%')\n",
    "print(f'HLS Fidelity: {(hls_acc*2-1) * 100:.5}:%')\n",
    "\n",
    "notes[\"HLS Acc\"] = hls_acc\n",
    "notes[\"HLS Fidelity\"] = hls_acc*2-1\n",
    "with open(os.path.join(output_dir, 'notes.json'), 'w') as file:\n",
    "    json.dump(notes, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create testbench files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_nrows = 10\n",
    "bottom_nrows = 10\n",
    "\n",
    "y_pred_top = checkpoint_model.predict(X_test[:top_nrows])\n",
    "y_pred_bottom = checkpoint_model.predict(X_test[-bottom_nrows:])\n",
    "ykeras_pred = np.vstack((y_pred_top, y_pred_bottom))\n",
    "\n",
    "y_hls_top = hls_model.predict(np.ascontiguousarray(X_test[:top_nrows].astype(np.float32))) \n",
    "y_hls_bottom = hls_model.predict(np.ascontiguousarray(X_test[-bottom_nrows:].astype(np.float32))) \n",
    "yhls_pred = np.vstack((y_hls_top, y_hls_bottom))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_array_to_dat(data=X_test, top_nrows=top_nrows, bottom_nrows=bottom_nrows, filename=os.path.join(output_dir, 'tb_data/tb_input_features.dat'))\n",
    "save_array_to_dat(data=ykeras_pred, top_nrows=top_nrows, bottom_nrows=bottom_nrows, filename=os.path.join(output_dir, 'tb_data/ykeras_pred.dat'))\n",
    "save_array_to_dat(data=yhls_pred, top_nrows=top_nrows, bottom_nrows=bottom_nrows, filename=os.path.join(output_dir, 'tb_data/yhls_pred.dat'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collect traces and compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, hls_trace = hls_model.trace(np.ascontiguousarray(X_test.astype(np.float32))) \n",
    "keras_trace = hls4ml.model.profiling.get_ymodel_keras(checkpoint_model, X_test) \n",
    "\n",
    "print(f'HLS Keys: {hls_trace.keys()}')\n",
    "print(f'Keras Keys: {keras_trace.keys()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 0\n",
    "\n",
    "hls_layers = hls_trace.keys()\n",
    "keras_layers = list(keras_trace.keys())\n",
    "\n",
    "for layer in hls_trace.keys():\n",
    "    keras_layer = layer \n",
    "    hls_layer = layer \n",
    "    keras_layer, hls_layer = keras_trace[keras_layer], hls_trace[hls_layer]\n",
    "    try:\n",
    "        diff = np.average(np.abs(keras_layer - hls_layer ))\n",
    "        print(f'Layer(s): {list(hls_trace.keys())[idx]}', '\\t\\t', diff)\n",
    "        \n",
    "        plt.figure(figsize=(7, 5))\n",
    "\n",
    "        plt.scatter(hls_layer.flatten(), keras_layer.flatten())\n",
    "        min_x = min(keras_layer.min(), hls_layer.min())\n",
    "        max_x = min(keras_layer.max(), hls_layer.max())\n",
    "\n",
    "        onnx_min, onnx_max = keras_layer.flatten().min(), keras_layer.flatten().max()\n",
    "        hls_min, hls_max = hls_layer.flatten().min(), hls_layer.flatten().max()\n",
    "        \n",
    "        print(f'hls/keras min: {hls_min}/{onnx_min}')\n",
    "        print(f'hls/keras max: {hls_max}/{onnx_max}')\n",
    "        \n",
    "        plt.plot([min_x, max_x], [min_x, max_x], c='red')\n",
    "        plt.axhline(min_x, c='red')\n",
    "        plt.axhline(max_x, c='red')\n",
    "\n",
    "        plt.title(f'(hls) {list(hls_trace.keys())[idx]} -- (keras) {list(keras_trace.keys())[idx]}')\n",
    "        plt.xlabel(f'hls4ml - [{hls_min:.3f},  {hls_max:.3f}]')\n",
    "        plt.ylabel(f'keras - [{onnx_min:.3f},  {onnx_max:.3f}]')\n",
    "        plt.yscale('linear')\n",
    "        idx += 1\n",
    "    except Exception as e:\n",
    "        print(e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Synthesize "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hls_model.build(\n",
    "    csim=False,\n",
    "    synth=True,\n",
    "    cosim=False,\n",
    "    export=False,\n",
    "    vsynth=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check the report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hls4ml.report.read_vivado_report(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml4qick-test-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
