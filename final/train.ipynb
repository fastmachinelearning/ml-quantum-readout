{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import pickle\n",
    "import hashlib\n",
    "\n",
    "import keras\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from tensorflow.keras.losses import BinaryCrossentropy \n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "from qkeras.qlayers import QDense\n",
    "from qkeras.quantizers import ternary\n",
    "\n",
    "os.environ['PATH'] = os.environ['XILINX_VIVADO'] + '/bin:' + os.environ['PATH']\n",
    "keras.utils.set_random_seed(32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://zenodo.org/api/records/14427490/files-archive\n",
    "!unzip files-archive -d data\n",
    "# !rm files-archive # optional cleanup "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_dir = \"./data\"\n",
    "test_data_dir = \"./data\"\n",
    "start_location = 100\n",
    "window_size = 400\n",
    "end_window = start_location + window_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Loadning training split\"\"\"\n",
    "x_train_path = os.path.join(train_data_dir, f'0528_X_train_0_770.npy')\n",
    "y_train_path = os.path.join(train_data_dir, f'0528_y_train_0_770.npy')\n",
    "\n",
    "assert os.path.exists(x_train_path), f\"ERROR: File {x_train_path} does not exist.\"\n",
    "assert os.path.exists(y_train_path), f\"ERROR: File {y_train_path} does not exist.\"\n",
    "\n",
    "X_train_val = np.load(x_train_path)\n",
    "y_train_val = np.load(y_train_path)\n",
    "\n",
    "# Insure same dataset is loaded \n",
    "assert hashlib.md5(X_train_val).hexdigest() == 'b61226c86b7dee0201a9158455e08ffb',  \"Checksum failed. Wrong file was loaded or file may be corrupted.\"\n",
    "assert hashlib.md5(y_train_val).hexdigest() == 'c59ce37dc7c73d2d546e7ea180fa8d31',  \"Checksum failed. Wrong file was loaded or file may be corrupted.\"\n",
    "\n",
    "# Get readout window\n",
    "X_train_val = X_train_val[:,start_location*2:end_window*2]\n",
    "assert len(X_train_val[0]) == (end_window-start_location)*2, f\"ERROR: X_test sample size {len(X_train_val[0])} does not match (start window, end window) ({start_location},{end_window}) size.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Loading testing split\"\"\"\n",
    "x_test_path = os.path.join(test_data_dir, f'0528_X_test_0_770.npy')\n",
    "y_test_path = os.path.join(test_data_dir, f'0528_y_test_0_770.npy')\n",
    "\n",
    "assert os.path.exists(x_test_path), f\"ERROR: File {x_test_path} does not exist.\"\n",
    "assert os.path.exists(y_test_path), f\"ERROR: File {y_test_path} does not exist.\"\n",
    "\n",
    "X_test = np.load(x_test_path)\n",
    "y_test = np.load(y_test_path)\n",
    "\n",
    "# Insure same dataset is loaded \n",
    "assert hashlib.md5(X_test).hexdigest() == 'b7d85f42522a0a57e877422bc5947cde', \"Checksum failed. Wrong file was loaded or file may be corrupted.\"\n",
    "assert hashlib.md5(y_test).hexdigest() == '8c9cce1821372380371ade5f0ccfd4a2', \"Checksum failed. Wrong file was loaded or file may be corrupted.\"\n",
    "\n",
    "# Get readout window\n",
    "X_test = X_test[:,start_location*2:end_window*2]\n",
    "assert len(X_test[0]) == (end_window-start_location)*2, f\"ERROR: X_test sample size {len(X_test[0])} does not match (start window, end window) ({start_location},{end_window}) size.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct the model\n",
    " \n",
    "QKeras is \"Quantized Keras\" for deep heterogeneous quantization of ML models. We're using QDense layer instead of Dense. We're also training with model sparsity, since QKeras layers are prunable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential()\n",
    "model.add(QDense(\n",
    "    4, \n",
    "    activation=None, \n",
    "    name='fc1',\n",
    "    input_shape=(800,), \n",
    "    kernel_quantizer=ternary(),\n",
    "    bias_quantizer=ternary(),\n",
    "))\n",
    "model.add(BatchNormalization(name='batchnorm1'))\n",
    "model.add(QDense(\n",
    "    1, \n",
    "    name='fc2', \n",
    "    activation='sigmoid', \n",
    "    kernel_quantizer=ternary(),\n",
    "    bias_quantizer=ternary(),\n",
    "))\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_learning_rate = 1e-3\n",
    "validation_split = 0.05  # 45,000 sample size \n",
    "batch_size = 256\n",
    "epochs = 50\n",
    "early_stopping_patience = 10\n",
    "\n",
    "checkpoint_dir = f'checkpoints/'\n",
    "checkpoint_filename = f'qkeras_ckp_model_best.h5'\n",
    "ckp_filename = os.path.join(checkpoint_dir, checkpoint_filename)\n",
    "\n",
    "if os.path.exists(checkpoint_dir) == False:\n",
    "    print(f'Checkpoint directory {checkpoint_dir} does not exist.')\n",
    "    print('Creating directory...')\n",
    "    os.mkdir(checkpoint_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = False\n",
    "\n",
    "if train: \n",
    "    opt = Adam(learning_rate=init_learning_rate)\n",
    "    callbacks = [\n",
    "        EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            patience=early_stopping_patience,\n",
    "            restore_best_weights=True,\n",
    "        ),\n",
    "    ] \n",
    "    model.compile(\n",
    "        optimizer=opt, \n",
    "        loss=BinaryCrossentropy(from_logits=False), \n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    history = model.fit(\n",
    "        X_train_val, \n",
    "        y_train_val, \n",
    "        batch_size=batch_size,\n",
    "        epochs=epochs, \n",
    "        validation_split=validation_split, \n",
    "        shuffle=True, \n",
    "        callbacks=callbacks,\n",
    "    )\n",
    "    \n",
    "    model.save_weights(os.path.join(checkpoint_dir, 'qkeras_model_best.h5'))\n",
    "    # Save the history dictionary\n",
    "    with open(os.path.join(checkpoint_dir, f'qkeras_training_history.pkl'), 'wb') as f:\n",
    "        pickle.dump(history.history, f)    \n",
    "else: \n",
    "    model.load_weights(os.path.join(checkpoint_dir, checkpoint_filename))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get ground and excited indices \n",
    "e_indices = np.where(y_test == 1)[0]\n",
    "g_indices = np.where(y_test == 0)[0]\n",
    "\n",
    "# separate ground and excited samples \n",
    "Xe_test = X_test[e_indices]\n",
    "ye_test = y_test[e_indices]\n",
    "\n",
    "Xg_test = X_test[g_indices]\n",
    "yg_test = y_test[g_indices]\n",
    "\n",
    "# compute total correct for excited state \n",
    "ye_pred = model.predict(Xe_test)\n",
    "ye_pred = np.where(ye_pred < 0.5, 0, 1).reshape(-1)\n",
    "e_accuracy = accuracy_score(ye_test, ye_pred)\n",
    "\n",
    "total_correct = (ye_test==ye_pred).astype(np.int8).sum()\n",
    "total_incorrect = (ye_test!=ye_pred).astype(np.int8).sum()\n",
    "\n",
    "# compute total correct for ground state \n",
    "yg_pred = model.predict(Xg_test)\n",
    "yg_pred = np.where(yg_pred < 0.5, 0, 1)\n",
    "g_accuracy = accuracy_score(yg_test, yg_pred)\n",
    "\n",
    "total_correct = (yg_test==yg_pred).astype(np.int8).sum()\n",
    "total_incorrect = (yg_test!=yg_pred).astype(np.int8).sum()\n",
    "\n",
    "# compute fidelity \n",
    "test_fidelity = 0.5*(e_accuracy + g_accuracy)\n",
    "test_fidelity = test_fidelity*2-1\n",
    "test_fidelity = 1/2 + (0.5*test_fidelity)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "test_acc = accuracy_score(y_test, np.where(y_pred < 0.5, 0, 1).reshape(-1))\n",
    "\n",
    "print('\\n===================================')\n",
    "print('    Accuracy', test_acc)\n",
    "print('    Fidelity', test_fidelity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml4qick-test-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
